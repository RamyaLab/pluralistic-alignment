{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, sys\n",
    "os.environ['CUDA_VISIBLE_DEVICES'] = '1'\n",
    "sys.path.append('..')\n",
    "from tqdm import tqdm\n",
    "import argparse\n",
    "from omegaconf import OmegaConf\n",
    "from types import MethodType\n",
    "\n",
    "import torch\n",
    "import lightning as L\n",
    "from src.pal_rm_b.lightningmodule import LearnerWrapLightning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-09-13 17:22:22,062 - src.pal_rm_b.learner - CRITICAL - ðŸ›‘ Remember to call update_trainable_params() after the model is initialized.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "the upper bound is 4.0\n"
     ]
    }
   ],
   "source": [
    "checkpoint = torch.load('/home/daiwei/projects/pal/refactored-diverse-alignment/ckpts/summary-b-seen-epoch=00-f7f8.ckpt')\n",
    "prefLearner_config = OmegaConf.load('../config/prefLearner_config/b-dim512-k2-opt350m-mlp2.yaml')\n",
    "optim_config = OmegaConf.load('../config/optim_config/vanilla-e1.yaml')\n",
    "loss_config = OmegaConf.load('../config/loss_config/b-cumulative.yaml')\n",
    "ds_config = OmegaConf.load('../config/ds_config/summary.yaml')\n",
    "uids = torch.load(ds_config.user_ids_path)\n",
    "state_dict = checkpoint['state_dict']\n",
    "\n",
    "pal = LearnerWrapLightning(prefLearner_config, optim_config, loss_config)\n",
    "pal.prefLearner.user_learner.init_weight(uids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_ckpt_learner(learner, ckpt_path):\n",
    "    state_dict = torch.load(ckpt_path,map_location='cpu')['state_dict']\n",
    "    learner.load_state_dict(state_dict)\n",
    "    return learner\n",
    "\n",
    "pal = load_ckpt_learner(pal, '/home/daiwei/projects/pal/refactored-diverse-alignment/ckpts/summary-b-seen-epoch=00-f7f8.ckpt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def wrap_mix_forward_b(pal: torch.nn.Module, mix_weight: torch.tensor):\n",
    "    # override the forward function of the pal to be a standard reward model\n",
    "    # after the modification, the pal will be able to output:\n",
    "    # model a: the reward diff given a prompt\n",
    "    # model b: the reward logits given a prompt\n",
    "    def mix_forward_userlearner(self, prompt_tokens):\n",
    "        prompt_logits = self.infer_gk(prompt_tokens)\n",
    "        bs = prompt_tokens['input_ids'].size(0)\n",
    "        w = self.softmax(mix_weight.repeat(bs, 1))\n",
    "        w = w.unsqueeze(-1).unsqueeze(-1)\n",
    "        y_hat = (w * prompt_logits).sum(dim=1)\n",
    "        self.tmp_store_user_ideal_points = y_hat\n",
    "        return y_hat\n",
    "    def mix_forward_itemlearner(self, items):\n",
    "        embeds = self.llm(input_ids=items['input_ids'], attention_mask=items['attention_mask']).last_hidden_state\n",
    "        # embeds shape: (bs, seq_len, hidden_size)\n",
    "        shape = embeds.shape\n",
    "        embeds = embeds.view(-1, shape[-1]) # (bs*seq_len, hidden_size)\n",
    "        projected_embeds = self.projector(embeds)\n",
    "        return projected_embeds.view(shape[0], shape[1], -1)\n",
    "    def mix_map_preflearner(self, x):\n",
    "        # ({\n",
    "        # 'input_ids': prompt_input_ids,\\\n",
    "        # 'attention_mask': prompt_attention_mask,\n",
    "        # },\\\n",
    "        # {\n",
    "        # 'input_ids': eval_input_ids,\\\n",
    "        # 'attention_mask': eval_attention_mask,\\\n",
    "        # })\n",
    "        prompt, items = x\n",
    "        items_prime = self.item_learner(items)\n",
    "        prompt_prime = self.user_learner(prompt)\n",
    "        return items_prime, prompt_prime\n",
    "    def mix_forward_preflearner(self, x):\n",
    "        items, prompt = x\n",
    "        items_prime, prompt_prime = self.map_to_pref_embedding_space((prompt, items))\n",
    "        print(f\"{items_prime[0]=}\")\n",
    "        print(f\"{prompt_prime[0]=}\")\n",
    "        print(f\"{items_prime.shape=}\")\n",
    "        print(f\"{prompt_prime.shape=}\")\n",
    "        if self.pref_learner_type == 'angle':\n",
    "            items_prime = items_prime / torch.norm(items_prime, dim=-1, keepdim=True)\n",
    "            prompt_prime = prompt_prime / torch.norm(prompt_prime, dim=-1, keepdim=True)\n",
    "            prompt_prime = prompt_prime.unsqueeze(1)\n",
    "            logit_scale = self.logit_scale.exp()\n",
    "            clamped_logit_scale = torch.clamp(logit_scale, max=100)\n",
    "            print(clamped_logit_scale)\n",
    "            print((prompt_prime * items_prime).sum(dim=-1))\n",
    "            sim_score = (prompt_prime * items_prime).sum(dim=-1) * clamped_logit_scale   # (bs, max_token_length)\n",
    "            return sim_score\n",
    "        else:\n",
    "            raise NotImplementedError\n",
    "    def forwad(self, batch):\n",
    "        y_hat = self.prefLearner(batch)\n",
    "        return y_hat\n",
    "    pal.prefLearner.user_learner.forward = MethodType(mix_forward_userlearner, pal.prefLearner.user_learner)\n",
    "    pal.prefLearner.item_learner.forward = MethodType(mix_forward_itemlearner, pal.prefLearner.item_learner)\n",
    "    pal.prefLearner.map_to_pref_embedding_space = MethodType(mix_map_preflearner, pal.prefLearner)\n",
    "    pal.prefLearner.forward = MethodType(mix_forward_preflearner, pal.prefLearner)\n",
    "    pal.forward = MethodType(forwad, pal)\n",
    "    return pal\n",
    "\n",
    "def wrap_mix_forward_a(pal: torch.nn.Module, mix_weight: torch.tensor):\n",
    "    # override the forward function of the pal to be a standard reward model\n",
    "    # after the modification, the pal will be able to output:\n",
    "    # model a: the reward diff given a prompt\n",
    "    # model b: the reward logits given a prompt\n",
    "    def mix_forward_userlearner(self):\n",
    "        w = self.softmax(mix_weight.unsqueeze(0))\n",
    "        if self.learner_type == 'f(Pw)':\n",
    "            res = self.projector(w @ self.P)\n",
    "            return res\n",
    "        elif self.learner_type == 'f(P)w':\n",
    "            res = w @ self.projector(self.P)\n",
    "            return res\n",
    "        elif self.learner_type == 'Pw':\n",
    "            res = w @ self.P\n",
    "            return res\n",
    "        else:\n",
    "            raise NotImplementedError\n",
    "    def mix_forward_itemlearner(self, items):\n",
    "        \n",
    "        ...\n",
    "    def mix_map_preflearner(self, x):\n",
    "        # {\n",
    "        # 'input_ids': input_ids,\\\n",
    "        # 'attention_mask': attention_mask,\\\n",
    "        # }\n",
    "        items = x\n",
    "        x_left_prime, x_right_prime = self.item_learner(items)\n",
    "        u_prime = self.user_learner()\n",
    "        bs = items['left_input_ids'].size(0)\n",
    "        u_prime = u_prime.repeat(bs, 1)\n",
    "        return x_left_prime, x_right_prime, u_prime\n",
    "    \n",
    "    def forwad(self, batch):\n",
    "        y_hat = self.prefLearner(batch)\n",
    "        return y_hat\n",
    "    pal.prefLearner.user_learner.forward = MethodType(mix_forward_userlearner, pal.prefLearner.user_learner)\n",
    "    pal.prefLearner.map_to_pref_embedding_space = MethodType(mix_map_preflearner, pal.prefLearner)\n",
    "    pal.forward = MethodType(forwad, pal)\n",
    "    return pal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "pal = wrap_mix_forward_b(pal, torch.tensor([0.5]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt_input_ids = torch.tensor([[101, 1045, 2515, 1037, 2515, 1012, 102]])\n",
    "prompt_attention_mask = torch.ones_like(prompt_input_ids)\n",
    "chosen_input_ids = torch.tensor([[101, 1045, 2515, 1037, 2515, 1012, 102]])\n",
    "chosen_attention_mask = torch.ones_like(chosen_input_ids)\n",
    "rejected_input_ids = torch.tensor([[101, 1045, 2515, 1037, 2515, 1012, 102]])\n",
    "rejected_attention_mask = torch.ones_like(rejected_input_ids)\n",
    "\n",
    "tmp = (\n",
    "    {\n",
    "    'input_ids': prompt_input_ids,\n",
    "    'attention_mask': prompt_attention_mask,\n",
    "    },\n",
    "    {\n",
    "    'input_ids': chosen_input_ids,\n",
    "    'attention_mask': chosen_attention_mask,\n",
    "    }\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "items_prime[0]=tensor([[ 0.1154, -0.2059, -0.0034,  ..., -1.0721,  0.2010,  0.1516],\n",
      "        [ 0.1147, -0.2062, -0.0038,  ..., -1.0724,  0.2012,  0.1545],\n",
      "        [ 0.1114, -0.2021, -0.0040,  ..., -1.0498,  0.1971,  0.1544],\n",
      "        ...,\n",
      "        [ 0.1092, -0.2000, -0.0043,  ..., -1.0371,  0.1949,  0.1557],\n",
      "        [ 0.1089, -0.1996, -0.0044,  ..., -1.0351,  0.1945,  0.1558],\n",
      "        [ 0.1078, -0.1984, -0.0045,  ..., -1.0283,  0.1933,  0.1561]],\n",
      "       grad_fn=<SelectBackward0>)\n",
      "prompt_prime[0]=tensor([[-0.0154, -0.0049, -0.0240,  ...,  0.0105,  0.0029,  0.1155],\n",
      "        [-0.0154, -0.0053, -0.0248,  ...,  0.0097,  0.0032,  0.1182],\n",
      "        [-0.0151, -0.0058, -0.0249,  ...,  0.0100,  0.0037,  0.1185],\n",
      "        ...,\n",
      "        [-0.0150, -0.0062, -0.0252,  ...,  0.0102,  0.0041,  0.1201],\n",
      "        [-0.0150, -0.0064, -0.0254,  ...,  0.0101,  0.0043,  0.1206],\n",
      "        [-0.0149, -0.0066, -0.0255,  ...,  0.0103,  0.0045,  0.1210]],\n",
      "       grad_fn=<SelectBackward0>)\n",
      "items_prime.shape=torch.Size([1, 7, 512])\n",
      "prompt_prime.shape=torch.Size([1, 7, 512])\n",
      "tensor(23.3060, grad_fn=<ClampBackward1>)\n",
      "tensor([[[-0.0440, -0.0298, -0.0121, -0.0090,  0.0057,  0.0082,  0.0155]]],\n",
      "       grad_fn=<SumBackward1>)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor([[[-1.0261, -0.6946, -0.2817, -0.2098,  0.1325,  0.1900,  0.3622]]],\n",
       "       grad_fn=<MulBackward0>)"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pal(tmp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(pal, 'tmp_model.pth')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(pal.state_dict(), 'tmp.pth')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-09-13 11:41:05,551 - src.pal_rm_b.learner - CRITICAL - ðŸ›‘ Remember to call update_trainable_params() after the model is initialized.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "the upper bound is 4.0\n"
     ]
    }
   ],
   "source": [
    "pal_a = LearnerWrapLightning(prefLearner_config, optim_config, loss_config)\n",
    "pal_a.prefLearner.user_learner.init_weight(uids)\n",
    "pal_a = wrap_mix_forward_b(pal_a, torch.tensor([0.5]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pal_a.load_state_dict(torch.load('tmp.pth'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "items_prime[0]=tensor([[-0.4623, -3.6934,  1.5608,  ..., -1.7927,  3.0306, -1.9023],\n",
      "        [-0.0801, -3.2334,  4.6325,  ...,  0.1047,  2.6710, -2.0856],\n",
      "        [-0.2887, -3.6674,  3.5159,  ..., -1.6389,  2.6698, -3.2263],\n",
      "        ...,\n",
      "        [ 0.1620,  1.5926,  1.5016,  ..., -1.9584,  2.0274, -2.0478],\n",
      "        [-0.1873,  4.0954,  1.5305,  ..., -1.8824,  1.8769, -0.5328],\n",
      "        [-0.3072,  2.4888,  2.3021,  ..., -0.5701,  1.8019, -1.1066]],\n",
      "       grad_fn=<SelectBackward0>)\n",
      "prompt_prime[0]=tensor([[-0.4616, -3.6943,  1.5638,  ..., -1.7951,  3.0320, -1.9020],\n",
      "        [-0.0793, -3.2343,  4.6356,  ...,  0.1023,  2.6724, -2.0853],\n",
      "        [-0.2879, -3.6683,  3.5189,  ..., -1.6413,  2.6711, -3.2259],\n",
      "        ...,\n",
      "        [ 0.1628,  1.5917,  1.5047,  ..., -1.9608,  2.0287, -2.0475],\n",
      "        [-0.1866,  4.0945,  1.5336,  ..., -1.8848,  1.8782, -0.5324],\n",
      "        [-0.3065,  2.4878,  2.3051,  ..., -0.5725,  1.8032, -1.1062]],\n",
      "       grad_fn=<SelectBackward0>)\n",
      "items_prime.shape=torch.Size([1, 7, 512])\n",
      "prompt_prime.shape=torch.Size([1, 7, 512])\n",
      "tensor(3.2628, grad_fn=<ClampBackward1>)\n",
      "tensor([[[1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000]]],\n",
      "       grad_fn=<SumBackward1>)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor([[[3.2628, 3.2628, 3.2628, 3.2628, 3.2628, 3.2628, 3.2628]]],\n",
       "       grad_fn=<MulBackward0>)"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pal_a(tmp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-09-13 11:41:06,979 - src.pal_rm_b.learner - CRITICAL - ðŸ›‘ Remember to call update_trainable_params() after the model is initialized.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "the upper bound is 4.0\n"
     ]
    }
   ],
   "source": [
    "pal_b = LearnerWrapLightning(prefLearner_config, optim_config, loss_config)\n",
    "pal_b.prefLearner.user_learner.init_weight(uids)\n",
    "pal_b = wrap_mix_forward_b(pal_b, torch.tensor([0.5]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_ckpt_learner(learner, ckpt_path):\n",
    "    state_dict = torch.load(ckpt_path,map_location='cpu')['state_dict']\n",
    "    learner.load_state_dict(state_dict)\n",
    "    return learner"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LearnerWrapLightning(\n",
       "  (prefLearner): PrefLearner_angle(\n",
       "    (llm): OPTModel(\n",
       "      (decoder): OPTDecoder(\n",
       "        (embed_tokens): Embedding(50272, 512, padding_idx=1)\n",
       "        (embed_positions): OPTLearnedPositionalEmbedding(2050, 1024)\n",
       "        (project_out): Linear(in_features=1024, out_features=512, bias=False)\n",
       "        (project_in): Linear(in_features=512, out_features=1024, bias=False)\n",
       "        (layers): ModuleList(\n",
       "          (0-23): 24 x OPTDecoderLayer(\n",
       "            (self_attn): OPTAttention(\n",
       "              (k_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (v_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (q_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            )\n",
       "            (activation_fn): ReLU()\n",
       "            (self_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "            (fc1): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "            (fc2): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "            (final_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (projector_f): Connector(\n",
       "      (mlp): Projector(\n",
       "        (identity): Identity()\n",
       "        (layers): Sequential(\n",
       "          (0): Linear(in_features=512, out_features=512, bias=True)\n",
       "          (1): Dropout(p=0.5, inplace=False)\n",
       "          (2): ReLU()\n",
       "          (3): Linear(in_features=512, out_features=512, bias=True)\n",
       "          (4): Dropout(p=0.5, inplace=False)\n",
       "          (5): ReLU()\n",
       "          (6): Linear(in_features=512, out_features=512, bias=True)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (softmax_w): CustomSoftMax()\n",
       "    (item_learner): ItemLearner(\n",
       "      (llm): OPTModel(\n",
       "        (decoder): OPTDecoder(\n",
       "          (embed_tokens): Embedding(50272, 512, padding_idx=1)\n",
       "          (embed_positions): OPTLearnedPositionalEmbedding(2050, 1024)\n",
       "          (project_out): Linear(in_features=1024, out_features=512, bias=False)\n",
       "          (project_in): Linear(in_features=512, out_features=1024, bias=False)\n",
       "          (layers): ModuleList(\n",
       "            (0-23): 24 x OPTDecoderLayer(\n",
       "              (self_attn): OPTAttention(\n",
       "                (k_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "                (v_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "                (q_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "                (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              )\n",
       "              (activation_fn): ReLU()\n",
       "              (self_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "              (fc1): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "              (fc2): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "              (final_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "            )\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "      (projector): Connector(\n",
       "        (mlp): Projector(\n",
       "          (identity): Identity()\n",
       "          (layers): Sequential(\n",
       "            (0): Linear(in_features=512, out_features=512, bias=True)\n",
       "            (1): Dropout(p=0.5, inplace=False)\n",
       "            (2): ReLU()\n",
       "            (3): Linear(in_features=512, out_features=512, bias=True)\n",
       "            (4): Dropout(p=0.5, inplace=False)\n",
       "            (5): ReLU()\n",
       "            (6): Linear(in_features=512, out_features=512, bias=True)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (user_learner): UserLearner(\n",
       "      (llm): OPTModel(\n",
       "        (decoder): OPTDecoder(\n",
       "          (embed_tokens): Embedding(50272, 512, padding_idx=1)\n",
       "          (embed_positions): OPTLearnedPositionalEmbedding(2050, 1024)\n",
       "          (project_out): Linear(in_features=1024, out_features=512, bias=False)\n",
       "          (project_in): Linear(in_features=512, out_features=1024, bias=False)\n",
       "          (layers): ModuleList(\n",
       "            (0-23): 24 x OPTDecoderLayer(\n",
       "              (self_attn): OPTAttention(\n",
       "                (k_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "                (v_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "                (q_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "                (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              )\n",
       "              (activation_fn): ReLU()\n",
       "              (self_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "              (fc1): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "              (fc2): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "              (final_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "            )\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "      (softmax): CustomSoftMax()\n",
       "      (W): ParameterDict(\n",
       "          (KZL1qeRzHNYSfDAuOctL1iyVV8WC5N): Parameter containing: [torch.FloatTensor of size 1]\n",
       "          (ZzGCcAhvqF0HnKxNsUjtJFadcZdyZj): Parameter containing: [torch.FloatTensor of size 1]\n",
       "          (p4Oh7rUGyLe1EpilJFWr9sPDpkO016): Parameter containing: [torch.FloatTensor of size 1]\n",
       "          (qo6WIyEh27cwAjWpA3Q60J7NaDxzQJ): Parameter containing: [torch.FloatTensor of size 1]\n",
       "          (zKV8BFGy60O0q7102ALF84S6Jo5i4q): Parameter containing: [torch.FloatTensor of size 1]\n",
       "          (i8YiBZlrYmlkkChr5b9BUKvDO6lR1d): Parameter containing: [torch.FloatTensor of size 1]\n",
       "          (M3icahkfAtC9CJrtKgQ7qvyZ5SD8wC): Parameter containing: [torch.FloatTensor of size 1]\n",
       "          (HNzkrs9geGu1YMMfZ5Qvdt0ZaCthfB): Parameter containing: [torch.FloatTensor of size 1]\n",
       "          (Jxv4hxfb9zTVa5nsMDFlnjSX5LZ8MK): Parameter containing: [torch.FloatTensor of size 1]\n",
       "          (UhQipwcpQmiGJmScocXOGOKyCBaFUg): Parameter containing: [torch.FloatTensor of size 1]\n",
       "      )\n",
       "      (projectors): ModuleDict(\n",
       "        (0): Connector(\n",
       "          (mlp): Projector(\n",
       "            (identity): Identity()\n",
       "            (layers): Sequential(\n",
       "              (0): Linear(in_features=512, out_features=512, bias=True)\n",
       "              (1): Dropout(p=0.5, inplace=False)\n",
       "              (2): ReLU()\n",
       "              (3): Linear(in_features=512, out_features=512, bias=True)\n",
       "              (4): Dropout(p=0.5, inplace=False)\n",
       "              (5): ReLU()\n",
       "              (6): Linear(in_features=512, out_features=512, bias=True)\n",
       "            )\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "load_ckpt_learner(pal_b, '../ckpts/summary-b-seen-epoch=02-bc92.ckpt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "items_prime[0]=tensor([[-0.4623, -3.6934,  1.5608,  ..., -1.7927,  3.0306, -1.9023],\n",
      "        [-0.0801, -3.2334,  4.6325,  ...,  0.1047,  2.6710, -2.0856],\n",
      "        [-0.2887, -3.6674,  3.5159,  ..., -1.6389,  2.6698, -3.2263],\n",
      "        ...,\n",
      "        [ 0.1620,  1.5926,  1.5016,  ..., -1.9584,  2.0274, -2.0478],\n",
      "        [-0.1873,  4.0954,  1.5305,  ..., -1.8824,  1.8769, -0.5328],\n",
      "        [-0.3072,  2.4888,  2.3021,  ..., -0.5701,  1.8019, -1.1066]],\n",
      "       grad_fn=<SelectBackward0>)\n",
      "prompt_prime[0]=tensor([[-0.4616, -3.6943,  1.5638,  ..., -1.7951,  3.0320, -1.9020],\n",
      "        [-0.0793, -3.2343,  4.6356,  ...,  0.1023,  2.6724, -2.0853],\n",
      "        [-0.2879, -3.6683,  3.5189,  ..., -1.6413,  2.6711, -3.2259],\n",
      "        ...,\n",
      "        [ 0.1628,  1.5917,  1.5047,  ..., -1.9608,  2.0287, -2.0475],\n",
      "        [-0.1866,  4.0945,  1.5336,  ..., -1.8848,  1.8782, -0.5324],\n",
      "        [-0.3065,  2.4878,  2.3051,  ..., -0.5725,  1.8032, -1.1062]],\n",
      "       grad_fn=<SelectBackward0>)\n",
      "items_prime.shape=torch.Size([1, 7, 512])\n",
      "prompt_prime.shape=torch.Size([1, 7, 512])\n",
      "tensor(3.2628, grad_fn=<ClampBackward1>)\n",
      "tensor([[[1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000]]],\n",
      "       grad_fn=<SumBackward1>)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor([[[3.2628, 3.2628, 3.2628, 3.2628, 3.2628, 3.2628, 3.2628]]],\n",
       "       grad_fn=<MulBackward0>)"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pal_b(tmp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "import logging\n",
    "logging.basicConfig(level=logging.WARNING, format='%(asctime)s - %(name)s - %(levelname)s - %(message)s')\n",
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "def load_pal_rm_b(\n",
    "    mix_weight: torch.tensor,\n",
    "    pref_Learner_config_path: str,\n",
    "    optim_config_path: str,\n",
    "    loss_config_path: str,\n",
    "    ds_config_path: str,\n",
    "    state_dict_path: str,\n",
    "    **kwargs,\n",
    "):\n",
    "    logger.critical(' ðŸ’  load configurations...')\n",
    "    prefLearner_config = OmegaConf.load(pref_Learner_config_path)\n",
    "    optim_config = OmegaConf.load(optim_config_path)\n",
    "    loss_config = OmegaConf.load(loss_config_path)\n",
    "    ds_config = OmegaConf.load(ds_config_path)\n",
    "    uids = torch.load(ds_config.user_ids_path)\n",
    "    logger.critical(' ðŸ’  initiaize pal_rm model...')\n",
    "    pal = LearnerWrapLightning(prefLearner_config, optim_config, loss_config)\n",
    "    pal.prefLearner.user_learner.init_weight(uids)\n",
    "    pal_rm = wrap_mix_forward_b(pal, mix_weight)\n",
    "    load_ckpt_learner(pal_rm, state_dict_path)\n",
    "    logger.critical(' ðŸ’  complete reformat: pal -> pal_rm!')\n",
    "    return pal_rm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-09-13 11:41:16,884 - __main__ - CRITICAL -  ðŸ’  load configurations...\n",
      "2024-09-13 11:41:16,895 - __main__ - CRITICAL -  ðŸ’  initiaize pal_rm model...\n",
      "2024-09-13 11:41:17,198 - src.pal_rm_b.learner - CRITICAL - ðŸ›‘ Remember to call update_trainable_params() after the model is initialized.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "the upper bound is 4.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-09-13 11:41:19,599 - __main__ - CRITICAL -  ðŸ’  complete reformat: pal -> pal_rm!\n"
     ]
    }
   ],
   "source": [
    "pal_c = load_pal_rm_b(\n",
    "    mix_weight=torch.tensor([0.5]),\n",
    "    pref_Learner_config_path='../config/prefLearner_config/b-dim512-k1-opt350m-mlp2.yaml',\n",
    "    optim_config_path='../config/optim_config/vanilla-e1.yaml',\n",
    "    loss_config_path='../config/loss_config/b-cumulative.yaml',\n",
    "    ds_config_path='../config/ds_config/summary.yaml',\n",
    "    state_dict_path='../ckpts/summary-b-seen-epoch=02-bc92.ckpt',\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "items_prime[0]=tensor([[-0.4623, -3.6934,  1.5608,  ..., -1.7927,  3.0306, -1.9023],\n",
      "        [-0.0801, -3.2334,  4.6325,  ...,  0.1047,  2.6710, -2.0856],\n",
      "        [-0.2887, -3.6674,  3.5159,  ..., -1.6389,  2.6698, -3.2263],\n",
      "        ...,\n",
      "        [ 0.1620,  1.5926,  1.5016,  ..., -1.9584,  2.0274, -2.0478],\n",
      "        [-0.1873,  4.0954,  1.5305,  ..., -1.8824,  1.8769, -0.5328],\n",
      "        [-0.3072,  2.4888,  2.3021,  ..., -0.5701,  1.8019, -1.1066]],\n",
      "       grad_fn=<SelectBackward0>)\n",
      "prompt_prime[0]=tensor([[-0.4616, -3.6943,  1.5638,  ..., -1.7951,  3.0320, -1.9020],\n",
      "        [-0.0793, -3.2343,  4.6356,  ...,  0.1023,  2.6724, -2.0853],\n",
      "        [-0.2879, -3.6683,  3.5189,  ..., -1.6413,  2.6711, -3.2259],\n",
      "        ...,\n",
      "        [ 0.1628,  1.5917,  1.5047,  ..., -1.9608,  2.0287, -2.0475],\n",
      "        [-0.1866,  4.0945,  1.5336,  ..., -1.8848,  1.8782, -0.5324],\n",
      "        [-0.3065,  2.4878,  2.3051,  ..., -0.5725,  1.8032, -1.1062]],\n",
      "       grad_fn=<SelectBackward0>)\n",
      "items_prime.shape=torch.Size([1, 7, 512])\n",
      "prompt_prime.shape=torch.Size([1, 7, 512])\n",
      "tensor(3.2628, grad_fn=<ClampBackward1>)\n",
      "tensor([[[1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000]]],\n",
      "       grad_fn=<SumBackward1>)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor([[[3.2628, 3.2628, 3.2628, 3.2628, 3.2628, 3.2628, 3.2628]]],\n",
       "       grad_fn=<MulBackward0>)"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pal_c(tmp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = torch.randn(1, 512)\n",
    "b = torch.randn(1, 512)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = a / torch.norm(a, dim=-1, keepdim=True)\n",
    "b = b / torch.norm(b, dim=-1, keepdim=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0.0603])"
      ]
     },
     "execution_count": 76,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(a * b).sum(dim=-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# check the group structure"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "W = pal.prefLearner.user_learner.W"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "W_extract = [W[i].detach().numpy() for i in W.keys()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<matplotlib.lines.Line2D at 0x7785560337a0>]"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe Kernel crashed while executing code in the current cell or a previous cell. \n",
      "\u001b[1;31mPlease review the code in the cell(s) to identify a possible cause of the failure. \n",
      "\u001b[1;31mClick <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import torch.nn.functional as F\n",
    "\n",
    "W_extract = np.stack(W_extract)\n",
    "W_extract = F.softmax(torch.tensor(W_extract), dim=1).numpy()\n",
    "\n",
    "minority = [3, 4, 5]\n",
    "majority = [0, 1, 2, 6, 7, 8, 9]\n",
    "\n",
    "# create color index, different for minority and majority\n",
    "color_idx = np.zeros(10)\n",
    "color_idx[minority] = 1\n",
    "color_idx[majority] = 0\n",
    "\n",
    "plt.scatter(W_extract[:, 0], W_extract[:, 1], c=color_idx)\n",
    "plt.plot([0, 1], [0, 1], '--')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pal",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
