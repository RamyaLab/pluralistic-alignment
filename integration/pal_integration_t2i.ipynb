{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8488f74b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, sys\n",
    "os.environ['CUDA_VISIBLE_DEVICES'] = '1'\n",
    "sys.path.append('..')\n",
    "from omegaconf import OmegaConf\n",
    "from types import MethodType\n",
    "\n",
    "import torch\n",
    "import lightning as L\n",
    "\n",
    "from src.pal_rm_b_t2i.lightningmodule import LearnerWrapLightning\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "7b389e58",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "the upper bound is 4\n"
     ]
    }
   ],
   "source": [
    "# init pms (through argparser)\n",
    "dname = 'pickapicv2'\n",
    "k = 1\n",
    "conf_learner = f'../config/prefLearner_config/t2i-b-dim1024-k{k}-general-embeddings-mlp2.yaml'\n",
    "conf_ds = f'../config/ds_config/{dname}_cliph_original_ds.yaml'\n",
    "cache = f'../necessary_cache/{dname}-dataset-tables'\n",
    "ckpt_path = f'../../pickapic_checkpts/'\n",
    "\n",
    "# load configs\n",
    "conf_learner = OmegaConf.load(conf_learner)\n",
    "conf_ds = OmegaConf.load(conf_ds)\n",
    "\n",
    "conf_learner.preference_learner_params.k = k\n",
    "conf_learner.max_epochs_new_pair = 50\n",
    "conf_ds.batch_size = 16384\n",
    "\n",
    "user_ids = torch.load(os.path.join(cache,\"user_ids.pt\"), weights_only=True)\n",
    "\n",
    "learner = LearnerWrapLightning(**conf_learner)\n",
    "learner.preference_learner.user_learner.init_weight(user_ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f50ee8d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "state_dict = torch.load(\n",
    "    f'{ckpt_path}/palb-{dname}-cliph-k{k}/seen-pickapicv2-cliph-modelB-angle-logistic-k{k}_trial0-1-16384-ca559ce4-epoch=05.ckpt',\n",
    "    map_location='cpu',\n",
    "    weights_only=True\n",
    ")['state_dict']\n",
    "\n",
    "def _tmp_state_dict_converter(state_dict):\n",
    "    new_state_dict = {}\n",
    "    for k, v in state_dict.items():\n",
    "        if '.m.' in k:\n",
    "            new_state_dict[k.replace('.m.', '.mlp.')] = v\n",
    "        else:\n",
    "            new_state_dict[k] = v\n",
    "    return new_state_dict\n",
    "\n",
    "learner.load_state_dict(_tmp_state_dict_converter(state_dict))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "675b11f1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dict_keys(['epoch', 'global_step', 'pytorch-lightning_version', 'state_dict', 'loops', 'callbacks', 'optimizer_states', 'lr_schedulers'])\n"
     ]
    }
   ],
   "source": [
    "model = torch.load(f'{ckpt_path}/palb-{dname}-cliph-k{k}/seen-pickapicv2-cliph-modelB-angle-logistic-k{k}_trial0-1-16384-ca559ce4-epoch=05.ckpt',\n",
    "                   map_location='cpu',\n",
    "                   weights_only=True)\n",
    "\n",
    "print(model.keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "cd39d5d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def wrap_mix_forward_b(pal: torch.nn.Module, mix_weight: torch.tensor):\n",
    "    # override the forward pass of the original model to be a standard reward model\n",
    "    # after the modification, the PAL reward model will output:\n",
    "    # PAL-A: the reward difference given a prompt\n",
    "    # PAL-B: the reward logits given a prompt\n",
    "\n",
    "    def mix_forward_userlearner(self, latent_prompts):\n",
    "        prompt_logits = self.infer_gk(latent_prompts)   # (bs, k, dims)\n",
    "        bs = prompt_logits.size(0)\n",
    "        w = self.softmax(mix_weight.repeat(bs, 1))  # (bs, k)\n",
    "        w = w.unsqueeze(1)  # (bs, 1, k)\n",
    "        y_hat = torch.bmm(w, prompt_logits) # (bs, 1, dims)\n",
    "        y_hat = y_hat.squeeze(1)    # (bs, dims)\n",
    "        self.tmp_store_user_ideal_points = y_hat\n",
    "        return y_hat\n",
    "    \n",
    "    def mix_forward_itemlearner(self, items):\n",
    "        x = self.connector_x(items)\n",
    "        if self.learner_type in ['dist','dist_normalization','angle','dist_logistic','angle_hinge']:  # |f(x)-f(u)|_2 or <f(x), f(u)>\n",
    "            return self.projector(x)\n",
    "        elif self.learner_type == 'norm':   # # |f(x-u)|_2 (do the self.projector() part in the PreferenceLearner)\n",
    "            return x\n",
    "        else:\n",
    "            raise ValueError(f\"Unknown learner_type={self.learner_type}.\")\n",
    "        \n",
    "    def mix_map_preflearner(self, x):\n",
    "        prompt, items = x\n",
    "        items_prime = self.item_learner(items)\n",
    "        prompt_prime = self.user_learner(prompt)\n",
    "        return items_prime, prompt_prime\n",
    "    \n",
    "    def mix_forward_preflearner(self, x):\n",
    "        items, prompt = x\n",
    "        items_prime, prompt_prime = self.map_to_pref_embedding_space((prompt, items))\n",
    "        print(f\"{items_prime[0]=}\")\n",
    "        print(f\"{prompt_prime[0]=}\")\n",
    "        print(f\"{items_prime.shape=}\")\n",
    "        print(f\"{prompt_prime.shape=}\")\n",
    "        if self.pref_learner_type == 'angle':\n",
    "            items_prime = items_prime / torch.norm(items_prime, dim=-1, keepdim=True)\n",
    "            prompt_prime = prompt_prime / torch.norm(prompt_prime, dim=-1, keepdim=True)\n",
    "            prompt_prime = prompt_prime.unsqueeze(1)\n",
    "            logit_scale = self.logit_scale.exp()\n",
    "            clamped_logit_scale = torch.clamp(logit_scale, max=100)\n",
    "            # print(clamped_logit_scale)\n",
    "            # print((prompt_prime * items_prime).sum(dim=-1))\n",
    "            sim_score = (prompt_prime * items_prime).sum(dim=-1) * clamped_logit_scale   # (bs, max_token_length)\n",
    "            return sim_score\n",
    "        else:\n",
    "            raise NotImplementedError\n",
    "        \n",
    "    def forward(self, batch):\n",
    "        y_hat = self.preference_learner(batch)\n",
    "        return y_hat\n",
    "    \n",
    "    pal.preference_learner.user_learner.forward = MethodType(mix_forward_userlearner, pal.preference_learner.user_learner)\n",
    "    pal.preference_learner.item_learner.forward = MethodType(mix_forward_itemlearner, pal.preference_learner.item_learner)\n",
    "    pal.preference_learner.map_to_pref_embedding_space = MethodType(mix_map_preflearner, pal.preference_learner)\n",
    "    pal.preference_learner.forward = MethodType(mix_forward_preflearner, pal.preference_learner)\n",
    "    pal.forward = MethodType(forward, pal)\n",
    "    \n",
    "    return pal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "8288d02b",
   "metadata": {},
   "outputs": [],
   "source": [
    "pal = wrap_mix_forward_b(learner, torch.tensor([0.5]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "a8327652",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example images from HPS v2 interactive dataset tool: https://tgxs002.github.io/hpd_test_vis/\n",
    "img_0_url = 'https://tgxs002.github.io/hpd_test_vis/static/assets/03079.jpg'\n",
    "img_1_url = 'https://tgxs002.github.io/hpd_test_vis/static/assets/03071.jpg'\n",
    "prompt = 'A person holding a very small slice on pizza between their fingers.'\n",
    "\n",
    "from IPython.display import Image, display\n",
    "import requests\n",
    "\n",
    "img_0 = requests.get(img_0_url).content\n",
    "img_1 = requests.get(img_1_url).content\n",
    "\n",
    "image0 = Image(img_0)\n",
    "image1 = Image(img_1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "9f8054fc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "items_prime[0]=tensor([-1.7085,  1.2021,  1.6133,  ...,  0.0625,  0.1212, -0.2356],\n",
      "       grad_fn=<SelectBackward0>)\n",
      "prompt_prime[0]=tensor([ 1.1747,  0.1154, -1.5194,  ...,  1.6674, -1.8732, -0.7937],\n",
      "       grad_fn=<SelectBackward0>)\n",
      "items_prime.shape=torch.Size([1, 1024])\n",
      "prompt_prime.shape=torch.Size([1, 1024])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor([[-2.6417]], grad_fn=<MulBackward0>)"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tmp_prompt_latent = torch.randn(1, 1024)\n",
    "tmp_item_latent = torch.randn(1, 1024)\n",
    "pal([tmp_prompt_latent, tmp_item_latent])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pal",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
