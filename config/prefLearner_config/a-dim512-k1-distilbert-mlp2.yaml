d_hid: 768
d_pref: 768
k: 1
llm_name: 'distilbert/distilbert-base-cased'
user_learner_type: 'f(Pw)'
pref_learner_type: 'dist'
proj_arch: 'mlp2-relu-dropout0-residual'
initializer_type: 'gaussian'
is_expectation_norm_init: False
sfx_type: 'softmax'
sfx_temperature: 1.
is_temperature_learnable: False
is_gumbel_hard: None
seed: 42
bs: 4